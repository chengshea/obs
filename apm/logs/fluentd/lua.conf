<system>
  # The default log level is info, and Fluentd outputs info, warn, error and fatal
  log_level info
</system>

# path 不支持复杂正则
<source>
  @type tail
  path /var/lib/docker/containers/*/*-json.log
  pos_file /var/log/td-agent/fluentd-docker.pos
  pos_file_compaction_interval 12h
  tag docker.*
  refresh_interval 18
  skip_refresh_on_startup true
  <parse>
    @type json
  </parse>
</source>

# https://github.com/fabric8io/fluent-plugin-docker_metadata_filter
<filter docker.var.lib.docker.containers.*.*.log>
  @type docker_metadata
</filter>

# 过滤处理
<filter docker.var.lib.docker.containers.*.*.log>
  @type grep
  # 采集指定容器
  <regexp>
    key $.docker.name
    pattern nginx|kafka
  </regexp>
</filter>



<filter docker.var.lib.docker.containers.*.*.log>
  @type record_transformer
  enable_ruby true
  <record>
    # 重命名key,并去掉value的/
    container_name ${record["docker"]["name"].gsub('/', '')}
    topic_name ${'fluent_' + record["docker"]["name"] || 'fluent_logs-default'}
  </record>
  remove_keys $.docker
</filter>
# remove_keys $.docker.labels,$.docker.image_id,$.docker.container_hostname,$.docker.id,log


<match docker.**>
  @type rdkafka2

  # https://github.com/fluent/fluent-plugin-kafka?tab=readme-ov-file#rdkafka-based-output-plugin
  brokers kafka:29092
  #topic 'fluent_fluentd'
  topic_key topic_name
  message_key_key (string) :default => 'message_key'


  <format>
    @type json
  </format>

  # Optional. See https://docs.fluentd.org/v/1.0/configuration/inject-section
  <inject>
    tag_key tag
    time_key time
  </inject>

  # See fluentd document for buffer section parameters: https://docs.fluentd.org/v/1.0/configuration/buffer-section
  # Buffer chunk key should be same with topic_key. If value is not found in the record, default_topic is used.
  <buffer topic_name,tag>
    @type memory
    flush_at_shutdown true
    flush_mode interval
    flush_interval 1s
    chunk_limit_size 3MB
    chunk_full_threshold 1
    total_limit_size 1024MB
    overflow_action block
  </buffer>

  max_send_retries 1
  required_acks -1
  ack_timeout 3
  compression_codec gzip

  # You can set any rdkafka configuration via this parameter: https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
  # https://github.com/fluent/fluent-plugin-kafka/issues/414
  rdkafka_options {
    "log_level" : 7,
    "request.timeout.ms": 5000,
    "security.protocol": "SASL_PLAINTEXT",
    "sasl.mechanism": "PLAIN",
    "sasl.username": "admin",
    "sasl.password": "admin-secret"
  }


  rdkafka_delivery_handle_poll_timeout 9
  max_send_limit_bytes 10485760
  max_enqueue_bytes_per_second 1048576
  unrecoverable_error_codes ["topic_authorization_failed", "msg_size_too_large"]
</match>

